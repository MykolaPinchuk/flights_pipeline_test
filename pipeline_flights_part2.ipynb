{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64befc0f-ee9f-42ef-b435-b854bf98ed86",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Simple example of Kubeflow pipeline to predict flight delays\n",
    "Code is borrowed from https://aiinpractice.com/gcp-mlops-vertex-ai-pipeline-scikit-learn/ and\n",
    "https://aiinpractice.com/gcp-mlops-vertex-ai-feature-store/\n",
    "\n",
    "I takes around 10 min to run the pipeline. At the end of the notebook see how to use prediction endpoint.\n",
    "\n",
    "\n",
    "Project works, but there are weird bugs with the bucket and data file. For some reason, only mpg3-temp-data works for now. \n",
    "\n",
    "#### Notes:\n",
    "- Project works, but there are weird bugs with the bucket and data file. For some reason, only mpg3-temp-data works for now. \n",
    "- There are often bugs when trying to create an endpoint with the same name as previously created and deleted endpoint in the same region.\n",
    "- The slowest part of the pipeline is deploying model to an endpoint. Using more powerful instance for an endpoint seems to speed up this step. after standard-8 more powerful insances seem to deploy slower. replica=3 seems to help too. Surprisingly, increasing replica count further makes model deployment slower. \n",
    "\n",
    "#### Next steps:\n",
    "1. Figure out how to use any bucket and any data file. Done\n",
    "2. Use more powerful instances to speed up all steps. Done, does not help much\n",
    "3. Go to the next part and add preprocessing pipeline.\n",
    "4. How to productionalize this pipeline?\n",
    "- (i) write daily cron job to try to pull new monthly data. when it succeeds, trigger this pipeline to retrain the model and save new nobthly perf-eval artifact.\n",
    "- (ii) simulate real-time user request daily. Use this to record daily perf-eval results dashboard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eef972-f743-4ad6-b013-7fe7d75e5f2f",
   "metadata": {},
   "source": [
    "#### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "058f9587-16fc-407c-afaf-f388f3666c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install Python 3.8 kernel into py38 environment\n",
    "# !bash startup.sh\n",
    "# solution from https://stackoverflow.com/questions/70535237/how-to-update-python-on-vertex-ai-notebooks,\n",
    "# answer by Joe Lee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fba76421-fc1b-4d93-b17b-8820d332c1bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.16'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from platform import python_version\n",
    "python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a622e2a-f0b0-45a6-9790-5f3ab987518e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AlreadyExists",
     "evalue": "409 Featurestore `flight_delays1` already exists in projects/polished-vault-379315/locations/us-central1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/conda_env/py38/lib/python3.8/site-packages/google/api_core/grpc_helpers.py:72\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/conda_env/py38/lib/python3.8/site-packages/grpc/_channel.py:1030\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1028\u001b[0m state, call, \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(request, timeout, metadata, credentials,\n\u001b[1;32m   1029\u001b[0m                               wait_for_ready, compression)\n\u001b[0;32m-> 1030\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda_env/py38/lib/python3.8/site-packages/grpc/_channel.py:910\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 910\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.ALREADY_EXISTS\n\tdetails = \"Featurestore `flight_delays1` already exists in projects/polished-vault-379315/locations/us-central1.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:173.194.192.95:443 {created_time:\"2023-04-24T14:45:16.095536264+00:00\", grpc_status:6, grpc_message:\"Featurestore `flight_delays1` already exists in projects/polished-vault-379315/locations/us-central1.\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAlreadyExists\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m time0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      9\u001b[0m aip\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39mPROJECT_ID, staging_bucket\u001b[38;5;241m=\u001b[39mBUCKET, location\u001b[38;5;241m=\u001b[39mREGION)\n\u001b[0;32m---> 11\u001b[0m flight_delays_feature_store \u001b[38;5;241m=\u001b[39m \u001b[43maip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFeaturestore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflight_delays1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monline_store_fixed_node_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m flight_entity_type \u001b[38;5;241m=\u001b[39m flight_delays_feature_store\u001b[38;5;241m.\u001b[39mcreate_entity_type(\n\u001b[1;32m     16\u001b[0m     entity_type_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflight\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlight entity\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m flight_entity_type\u001b[38;5;241m.\u001b[39mbatch_create_features(\n\u001b[1;32m     21\u001b[0m     {\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morigin_airport_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     }\n\u001b[1;32m     47\u001b[0m )\n",
      "File \u001b[0;32m~/conda_env/py38/lib/python3.8/site-packages/google/cloud/aiplatform/base.py:814\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    813\u001b[0m         VertexAiResourceNounWithFutureManager\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[1;32m    817\u001b[0m internal_callbacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/conda_env/py38/lib/python3.8/site-packages/google/cloud/aiplatform/featurestore/featurestore.py:520\u001b[0m, in \u001b[0;36mFeaturestore.create\u001b[0;34m(cls, featurestore_id, online_store_fixed_node_count, labels, project, location, credentials, request_metadata, encryption_spec_key_name, sync, create_request_timeout)\u001b[0m\n\u001b[1;32m    512\u001b[0m     gapic_featurestore\u001b[38;5;241m.\u001b[39mencryption_spec \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    513\u001b[0m         initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mget_encryption_spec(\n\u001b[1;32m    514\u001b[0m             encryption_spec_key_name\u001b[38;5;241m=\u001b[39mencryption_spec_key_name\n\u001b[1;32m    515\u001b[0m         )\n\u001b[1;32m    516\u001b[0m     )\n\u001b[1;32m    518\u001b[0m api_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_instantiate_client(location\u001b[38;5;241m=\u001b[39mlocation, credentials\u001b[38;5;241m=\u001b[39mcredentials)\n\u001b[0;32m--> 520\u001b[0m created_featurestore_lro \u001b[38;5;241m=\u001b[39m \u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_featurestore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobal_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommon_location_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocation\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeaturestore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgapic_featurestore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeaturestore_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeaturestore_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mlog_create_with_lro(\u001b[38;5;28mcls\u001b[39m, created_featurestore_lro)\n\u001b[1;32m    532\u001b[0m created_featurestore \u001b[38;5;241m=\u001b[39m created_featurestore_lro\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[0;32m~/conda_env/py38/lib/python3.8/site-packages/google/cloud/aiplatform_v1/services/featurestore_service/client.py:644\u001b[0m, in \u001b[0;36mFeaturestoreServiceClient.create_featurestore\u001b[0;34m(self, request, parent, featurestore, featurestore_id, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    639\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(metadata) \u001b[38;5;241m+\u001b[39m (\n\u001b[1;32m    640\u001b[0m     gapic_v1\u001b[38;5;241m.\u001b[39mrouting_header\u001b[38;5;241m.\u001b[39mto_grpc_metadata(((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparent\u001b[39m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mparent),)),\n\u001b[1;32m    641\u001b[0m )\n\u001b[1;32m    643\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;66;03m# Wrap the response in an operation future.\u001b[39;00m\n\u001b[1;32m    652\u001b[0m response \u001b[38;5;241m=\u001b[39m gac_operation\u001b[38;5;241m.\u001b[39mfrom_gapic(\n\u001b[1;32m    653\u001b[0m     response,\n\u001b[1;32m    654\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39moperations_client,\n\u001b[1;32m    655\u001b[0m     gca_featurestore\u001b[38;5;241m.\u001b[39mFeaturestore,\n\u001b[1;32m    656\u001b[0m     metadata_type\u001b[38;5;241m=\u001b[39mfeaturestore_service\u001b[38;5;241m.\u001b[39mCreateFeaturestoreOperationMetadata,\n\u001b[1;32m    657\u001b[0m )\n",
      "File \u001b[0;32m~/conda_env/py38/lib/python3.8/site-packages/google/api_core/gapic_v1/method.py:113\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m     metadata\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata)\n\u001b[1;32m    111\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m metadata\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda_env/py38/lib/python3.8/site-packages/google/api_core/grpc_helpers.py:74\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mAlreadyExists\u001b[0m: 409 Featurestore `flight_delays1` already exists in projects/polished-vault-379315/locations/us-central1."
     ]
    }
   ],
   "source": [
    "import time\n",
    "from google.cloud import aiplatform as aip\n",
    "PROJECT_ID = \"polished-vault-379315\"  \n",
    "REGION = \"us-central1\"\n",
    "BUCKET = 'mpg3-testflights-polished-vault-379315'\n",
    "\n",
    "time0 = time.time()\n",
    "\n",
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET, location=REGION)\n",
    "\n",
    "flight_delays_feature_store = aip.Featurestore.create(\n",
    "    \"flight_delays1\", online_store_fixed_node_count=1\n",
    ")\n",
    "\n",
    "flight_entity_type = flight_delays_feature_store.create_entity_type(\n",
    "    entity_type_id=\"flight\",\n",
    "    description=\"Flight entity\",\n",
    ")\n",
    "\n",
    "flight_entity_type.batch_create_features(\n",
    "    {\n",
    "        \"origin_airport_id\": {\n",
    "            \"value_type\": \"STRING\",\n",
    "            \"description\": \"Airport ID for the origin\",\n",
    "        },\n",
    "        \"is_cancelled\": {\n",
    "            \"value_type\": \"BOOL\",\n",
    "            \"description\": \"Has the flight been cancelled or diverted?\",\n",
    "        },\n",
    "        \"departure_delay_minutes\": {\n",
    "            \"value_type\": \"DOUBLE\",\n",
    "            \"description\": \"Departure delay in minutes\",\n",
    "        },\n",
    "        \"arrival_delay_minutes\": {\n",
    "            \"value_type\": \"DOUBLE\",\n",
    "            \"description\": \"Arrival delay in minutes\",\n",
    "        },\n",
    "        \"taxi_out_minutes\": {\n",
    "            \"value_type\": \"DOUBLE\",\n",
    "            \"description\": \"Taxi out time in minutes\",\n",
    "        },\n",
    "        \"distance_miles\": {\n",
    "            \"value_type\": \"DOUBLE\",\n",
    "            \"description\": \"Total flight distance in miles.\",\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "airport_entity_type = flight_delays_feature_store.create_entity_type(\n",
    "    entity_type_id=\"airport\",\n",
    "    description=\"Airport entity\",\n",
    ")\n",
    "\n",
    "airport_entity_type.create_feature(\n",
    "    feature_id=\"average_departure_delay\",\n",
    "    value_type=\"DOUBLE\",\n",
    "    description=\"Average departure delay for that airport, calculated every 4h with 1h rolling window\",\n",
    ")\n",
    "\n",
    "print(f'Time to create FeatureStore: {time.time()-time0:.2f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d41dce0f-967c-4ff7-9cba-0e80fa1a7c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get featurestore_id from the above output. \n",
    "# if forgot, it may be easier to recreate fs with a new name.\n",
    "FEATURE_STORE_ID = \"662390005506\"\n",
    "# ENDPOINT_ID = \"xxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1726ac82-bcc1-41f1-abb5-76d9065dda8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_pipeline/feature_pipeline/helpers.py\n",
    "\n",
    "from typing import Union, get_args, get_origin\n",
    "from datetime import datetime\n",
    "\n",
    "def map_to_avro_type(field_type):\n",
    "    if field_type == str:\n",
    "        return \"string\"\n",
    "    elif field_type == bool:\n",
    "        return \"boolean\"\n",
    "    elif field_type == float:\n",
    "        return \"double\"\n",
    "    elif field_type is type(None):\n",
    "        return \"null\"\n",
    "    elif field_type == datetime:\n",
    "        return {\"type\": \"long\", \"logicalType\": \"timestamp-micros\"}\n",
    "    elif get_origin(field_type) == Union:\n",
    "        return [map_to_avro_type(t) for t in get_args(field_type)]\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unsupported type: {field_type}\")\n",
    "\n",
    "\n",
    "def named_tuple_to_avro_fields(named_tuple):\n",
    "    fields = []\n",
    "    for field_name, field_type in named_tuple.__annotations__.items():\n",
    "        fields.append({\"name\": field_name, \"type\": map_to_avro_type(field_type)})\n",
    "    return fields\n",
    "\n",
    "\n",
    "csv_headers = [\n",
    "    \"Year\",\n",
    "    \"Quarter\",\n",
    "    \"Month\",\n",
    "    \"DayofMonth\",\n",
    "    \"DayOfWeek\",\n",
    "    \"FlightDate\",\n",
    "    \"Reporting_Airline\",\n",
    "    \"DOT_ID_Reporting_Airline\",\n",
    "    \"IATA_CODE_Reporting_Airline\",\n",
    "    \"Tail_Number\",\n",
    "    \"Flight_Number_Reporting_Airline\",\n",
    "    \"OriginAirportID\",\n",
    "    \"OriginAirportSeqID\",\n",
    "    \"OriginCityMarketID\",\n",
    "    \"Origin\",\n",
    "    \"OriginCityName\",\n",
    "    \"OriginState\",\n",
    "    \"OriginStateFips\",\n",
    "    \"OriginStateName\",\n",
    "    \"OriginWac\",\n",
    "    \"DestAirportID\",\n",
    "    \"DestAirportSeqID\",\n",
    "    \"DestCityMarketID\",\n",
    "    \"Dest\",\n",
    "    \"DestCityName\",\n",
    "    \"DestState\",\n",
    "    \"DestStateFips\",\n",
    "    \"DestStateName\",\n",
    "    \"DestWac\",\n",
    "    \"CRSDepTime\",\n",
    "    \"DepTime\",\n",
    "    \"DepDelay\",\n",
    "    \"DepDelayMinutes\",\n",
    "    \"DepDel15\",\n",
    "    \"DepartureDelayGroups\",\n",
    "    \"DepTimeBlk\",\n",
    "    \"TaxiOut\",\n",
    "    \"WheelsOff\",\n",
    "    \"WheelsOn\",\n",
    "    \"TaxiIn\",\n",
    "    \"CRSArrTime\",\n",
    "    \"ArrTime\",\n",
    "    \"ArrDelay\",\n",
    "    \"ArrDelayMinutes\",\n",
    "    \"ArrDel15\",\n",
    "    \"ArrivalDelayGroups\",\n",
    "    \"ArrTimeBlk\",\n",
    "    \"Cancelled\",\n",
    "    \"CancellationCode\",\n",
    "    \"Diverted\",\n",
    "    \"CRSElapsedTime\",\n",
    "    \"ActualElapsedTime\",\n",
    "    \"AirTime\",\n",
    "    \"Flights\",\n",
    "    \"Distance\",\n",
    "    \"DistanceGroup\",\n",
    "    \"CarrierDelay\",\n",
    "    \"WeatherDelay\",\n",
    "    \"NASDelay\",\n",
    "    \"SecurityDelay\",\n",
    "    \"LateAircraftDelay\",\n",
    "    \"FirstDepTime\",\n",
    "    \"TotalAddGTime\",\n",
    "    \"LongestAddGTime\",\n",
    "    \"DivAirportLandings\",\n",
    "    \"DivReachedDest\",\n",
    "    \"DivActualElapsedTime\",\n",
    "    \"DivArrDelay\",\n",
    "    \"DivDistance\",\n",
    "    \"Div1Airport\",\n",
    "    \"Div1AirportID\",\n",
    "    \"Div1AirportSeqID\",\n",
    "    \"Div1WheelsOn\",\n",
    "    \"Div1TotalGTime\",\n",
    "    \"Div1LongestGTime\",\n",
    "    \"Div1WheelsOff\",\n",
    "    \"Div1TailNum\",\n",
    "    \"Div2Airport\",\n",
    "    \"Div2AirportID\",\n",
    "    \"Div2AirportSeqID\",\n",
    "    \"Div2WheelsOn\",\n",
    "    \"Div2TotalGTime\",\n",
    "    \"Div2LongestGTime\",\n",
    "    \"Div2WheelsOff\",\n",
    "    \"Div2TailNum\",\n",
    "    \"Div3Airport\",\n",
    "    \"Div3AirportID\",\n",
    "    \"Div3AirportSeqID\",\n",
    "    \"Div3WheelsOn\",\n",
    "    \"Div3TotalGTime\",\n",
    "    \"Div3LongestGTime\",\n",
    "    \"Div3WheelsOff\",\n",
    "    \"Div3TailNum\",\n",
    "    \"Div4Airport\",\n",
    "    \"Div4AirportID\",\n",
    "    \"Div4AirportSeqID\",\n",
    "    \"Div4WheelsOn\",\n",
    "    \"Div4TotalGTime\",\n",
    "    \"Div4LongestGTime\",\n",
    "    \"Div4WheelsOff\",\n",
    "    \"Div4TailNum\",\n",
    "    \"Div5Airport\",\n",
    "    \"Div5AirportID\",\n",
    "    \"Div5AirportSeqID\",\n",
    "    \"Div5WheelsOn\",\n",
    "    \"Div5TotalGTime\",\n",
    "    \"Div5LongestGTime\",\n",
    "    \"Div5WheelsOff\",\n",
    "    \"Div5TailNum\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c1b81af-0279-4c3c-b236-8be0234df233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "class Flight(NamedTuple):\n",
    "    timestamp: Optional[datetime]\n",
    "    flight_number: str\n",
    "    origin_airport_id: str\n",
    "    is_cancelled: bool\n",
    "    departure_delay_minutes: float\n",
    "    arrival_delay_minutes: float\n",
    "    taxi_out_minutes: float\n",
    "    distance_miles: float\n",
    "\n",
    "\n",
    "flight_avro_schema = {\n",
    "    \"namespace\": \"flight_delay_prediction\",\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Flight\",\n",
    "    \"fields\": named_tuple_to_avro_fields(Flight),\n",
    "}\n",
    "\n",
    "\n",
    "class AirportFeatures(NamedTuple):\n",
    "    timestamp: Optional[datetime]\n",
    "    origin_airport_id: str\n",
    "    average_departure_delay: float\n",
    "\n",
    "\n",
    "airport_avro_schema = {\n",
    "    \"namespace\": \"flight_delay_prediction\",\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Airport\",\n",
    "    \"fields\": named_tuple_to_avro_fields(AirportFeatures),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d67a5b-3d35-4d4c-98d6-aeb3607e8c6e",
   "metadata": {},
   "source": [
    "#### 2.Build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c0c0fe6-5a1a-410f-84a4-f64a178618a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_feature_pipeline.py\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "\n",
    "\n",
    "def parse_csv(line: str):\n",
    "    import csv\n",
    "    return next(csv.reader([line]))\n",
    "\n",
    "\n",
    "def parse_line(fields):\n",
    "    from datetime import datetime\n",
    "    from apache_beam.utils.timestamp import Timestamp\n",
    "\n",
    "    data = dict(zip(csv_headers, fields))\n",
    "\n",
    "    if (\n",
    "        data[\"Year\"] != \"Year\"  # skip header row\n",
    "        and len(data[\"WheelsOff\"]) == 4  #\n",
    "        and len(data[\"FlightDate\"]) == 10  # row has a flight date\n",
    "        and data[\"Distance\"] != \"\"\n",
    "    ):\n",
    "        wheels_off_hour = data[\"WheelsOff\"][:2]\n",
    "        wheels_off_minutes = data[\"WheelsOff\"][2:]\n",
    "        departure_date_time = (\n",
    "            f\"{data['FlightDate']}T{wheels_off_hour}:{wheels_off_minutes}:00\"\n",
    "        )\n",
    "\n",
    "        cancelled = (float(data[\"Cancelled\"]) > 0) or (float(data[\"Diverted\"]) > 0)\n",
    "\n",
    "        try:\n",
    "            flight = Flight(\n",
    "                timestamp=datetime.fromisoformat(departure_date_time),\n",
    "                origin_airport_id=str(data[\"OriginAirportID\"]),\n",
    "                flight_number=f\"{data['Reporting_Airline']}//{data['Flight_Number_Reporting_Airline']}\",\n",
    "                is_cancelled=cancelled,\n",
    "                departure_delay_minutes=float(data[\"DepDelay\"]),\n",
    "                arrival_delay_minutes=float(data[\"ArrDelay\"]),\n",
    "                taxi_out_minutes=float(data[\"TaxiOut\"]),\n",
    "                distance_miles=float(data[\"Distance\"]),\n",
    "            )\n",
    "\n",
    "            yield beam.window.TimestampedValue(\n",
    "                flight, Timestamp.from_rfc3339(departure_date_time)\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "class BuildTimestampedRecordFn(beam.DoFn):\n",
    "    def process(self, element, window=beam.DoFn.WindowParam):\n",
    "\n",
    "        window_start = window.start.to_utc_datetime()\n",
    "        return [\n",
    "            AirportFeatures(\n",
    "                timestamp=window_start,\n",
    "                origin_airport_id=element.origin_airport_id,\n",
    "                average_departure_delay=element.average_departure_delay,\n",
    "            )._asdict()\n",
    "        ]\n",
    "\n",
    "\n",
    "class BuildTimestampedFlightRecordFn(beam.DoFn):\n",
    "    def process(self, element: Flight, window=beam.DoFn.WindowParam):\n",
    "        return [element._asdict()]\n",
    "\n",
    "\n",
    "def run(argv=None, save_main_session=False):\n",
    "    \"\"\"Main entry point; defines and runs the wordcount pipeline.\n",
    "    never mind default arguments, they will not be invoked.\"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--input\",\n",
    "        dest=\"input\",\n",
    "        default=\"/Users/simon/projects/private/gcp_mlops/data/processed/2020/2020-05.csv\",\n",
    "        help=\"Input file to process.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output-airports\",\n",
    "        dest=\"output_airports\",\n",
    "        default=\"/Users/simon/projects/private/gcp_mlops/data/output_airports/\",\n",
    "        help=\"Output file to write results to.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--output-flights\",\n",
    "        dest=\"output_flights\",\n",
    "        default=\"/Users/simon/projects/private/gcp_mlops/data/output_flights/\",\n",
    "        help=\"Output file to write results to.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--output-read-instances\",\n",
    "        dest=\"output_read_instances\",\n",
    "        default=\"/Users/simon/projects/private/gcp_mlops/data/output_read_instances/\",\n",
    "        help=\"Output file to write results to.\",\n",
    "    )\n",
    "\n",
    "    # Parse beam arguments (e.g. --runner=DirectRunner to run the pipeline locally)\n",
    "    known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "\n",
    "    # We use the save_main_session option because one or more DoFn's in this\n",
    "    # workflow rely on global context (e.g., a module imported at module level).\n",
    "    pipeline_options = PipelineOptions(pipeline_args)\n",
    "    pipeline_options.view_as(SetupOptions).save_main_session = save_main_session\n",
    "\n",
    "    with beam.Pipeline(options=pipeline_options) as pipeline:\n",
    "        flights = (\n",
    "            pipeline\n",
    "            | \"read_input\" >> beam.io.ReadFromText(known_args.input)\n",
    "            | \"parse_csv\" >> beam.Map(parse_csv)\n",
    "            | \"create_flight_obj\" >> beam.FlatMap(parse_line).with_output_types(Flight)\n",
    "        )\n",
    "\n",
    "        # Create airport data\n",
    "        (\n",
    "            flights\n",
    "            | \"window\"\n",
    "            >> beam.WindowInto(\n",
    "                beam.window.SlidingWindows(4 * 60 * 60, 60 * 60)\n",
    "            )  # 4h time windows, every 60min\n",
    "            | \"group_by_airport\"\n",
    "            >> beam.GroupBy(\"origin_airport_id\").aggregate_field(\n",
    "                \"departure_delay_minutes\",\n",
    "                beam.combiners.MeanCombineFn(),\n",
    "                \"average_departure_delay\",\n",
    "            )\n",
    "            | \"add_timestamp\" >> beam.ParDo(BuildTimestampedRecordFn())\n",
    "            | \"write_airport_data\"\n",
    "            >> beam.io.WriteToAvro(\n",
    "                known_args.output_airports, schema=airport_avro_schema\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Create flight data\n",
    "        (\n",
    "            flights\n",
    "            | \"format_output\" >> beam.ParDo(BuildTimestampedFlightRecordFn())\n",
    "            | \"write_flight_data\"\n",
    "            >> beam.io.WriteToAvro(known_args.output_flights, schema=flight_avro_schema)\n",
    "        )\n",
    "\n",
    "        # Create read_instances.csv to retrieve training data from the feature store\n",
    "        (\n",
    "            flights\n",
    "            | \"format_read_instances_output\"\n",
    "            >> beam.Map(\n",
    "                lambda flight: f\"{flight.flight_number},{flight.origin_airport_id},{flight.timestamp.isoformat('T') + 'Z'}\"\n",
    "            )\n",
    "            | \"write_read_instances\"\n",
    "            >> beam.io.WriteToText(\n",
    "                known_args.output_read_instances,\n",
    "                file_name_suffix=\".csv\",\n",
    "                num_shards=1,\n",
    "                header=\"flight,airport,timestamp\",\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d5fbb5-8fc7-4bc0-9ec7-597db3b63315",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "run() got an unexpected keyword argument 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# run this pipeline\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# !bash pipeline_run.sh\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgs://\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mBUCKET\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/data/processed/2020/2020-05.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_flights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgs://\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mBUCKET\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/features/flight_features\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_airports\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgs://\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mBUCKET\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/features/airport_features/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_read_instances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgs://\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mBUCKET\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/features/read_instances/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDataflowRunner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPROJECT_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mREGION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstaging_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgs://\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mBUCKET\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/beam_staging\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemp_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgs://\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mBUCKET\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/beam_tmp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflight-batch-features\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: run() got an unexpected keyword argument 'input'"
     ]
    }
   ],
   "source": [
    "# run this pipeline\n",
    "\n",
    "# !bash pipeline_run.sh\n",
    "\n",
    "python ./main.py \\\n",
    "    --input=gs://${BUCKET}/data/processed/2020/2020-05.csv \\\n",
    "    --output-flights=gs://${BUCKET}/features/flight_features/ \\\n",
    "    --output-airports=gs://${BUCKET}/features/airport_features/ \\\n",
    "    --output-read-instances=gs://${BUCKET}/features/read_instances/ \\\n",
    "    --runner=DataflowRunner \\\n",
    "    --project=${PROJECT_ID} \\\n",
    "    --region=us-central1 \\\n",
    "    --staging_location=gs://${BUCKET}/beam_staging \\\n",
    "    --temp_location=gs://${BUCKET}/beam_tmp \\\n",
    "    --job_name=flight-batch-features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddead7c9-9ab4-4e70-ad90-792094314ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://mpg3-testflights-polished-vault-379315/data/processed/2020/2020-05.csv'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462c17b9-4504-477c-a46d-a8c60d6f4f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-central1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1c9cc1-9a12-4c16-a933-0938b505b59a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2567127497.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    import apache-beam\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bddf2e8-5bc9-4aac-829d-5fecd4151532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingest batch features:\n",
    "\n",
    "from google.cloud import aiplatform as aip\n",
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET, location=REGION)\n",
    "\n",
    "flight_delays_feature_store = aip.Featurestore(\n",
    "    FEATURE_STORE_ID,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")\n",
    "\n",
    "flight_entity_type = flight_delays_feature_store.get_entity_type(\"flight\")\n",
    "flight_entity_type.ingest_from_gcs(\n",
    "    feature_ids=[\n",
    "        \"origin_airport_id\",\n",
    "        \"is_cancelled\",\n",
    "        \"departure_delay_minutes\",\n",
    "        \"arrival_delay_minutes\",\n",
    "        \"taxi_out_minutes\",\n",
    "        \"distance_miles\",\n",
    "    ],\n",
    "    feature_time=\"timestamp\",\n",
    "    gcs_source_uris=f\"gs://{BUCKET}/features/flight_features/*\",\n",
    "    gcs_source_type=\"avro\",\n",
    "    entity_id_field=\"flight_number\",\n",
    ")\n",
    "\n",
    "airport_entity_type = flight_delays_feature_store.get_entity_type(\"airport\")\n",
    "airport_entity_type.ingest_from_gcs(\n",
    "    feature_ids=[\"average_departure_delay\"],\n",
    "    feature_time=\"timestamp\",\n",
    "    gcs_source_uris=f\"gs://{BUCKET}/features/airport_features/*\",\n",
    "    gcs_source_type=\"avro\",\n",
    "    entity_id_field=\"origin_airport_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f058cbb8-392a-4d46-9989-344560f45fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9333d6-bda9-43f6-a524-e6e83ba65836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c297587f-463c-421f-bc61-b4246271c5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2135ab10-49df-4da6-8d46-bc437414df5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79ee811-9b3b-4ad7-a34e-0d580c8f4264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b541e91-efc0-49d1-b479-130d88b5dbc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd45b9b-2db9-4f78-a776-640553206293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e20e3e5-4445-4019-8afe-01c414dd80f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bcef887-b1bb-4db4-bf88-07874ad61b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next step should be to run pipeline. in his repo, this is main.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd1d4b3-da69-4fca-b666-77cdc4dc77b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a02380-5d56-49fc-ad5d-6e9b51965509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667284bd-4f66-4de0-8eae-a52742fb8d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fe7af7-fcb8-41ca-a616-70baddda0b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "43e6922e-18a5-4247-92c1-fc9cacca2e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "USER_NAME=\"oo00011760@gmail.com\" \n",
    "PROJECT_ID = \"polished-vault-379315\"  \n",
    "REGION = \"us-central1\"\n",
    "# REGION = \"us-east1\"\n",
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c0864806-79c7-49fe-8bb8-dacf3c263433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Account: vertex-ai-service-account@polished-vault-379315.iam.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "SERVICE_ACCOUNT = 'vertex-ai-service-account@polished-vault-379315.iam.gserviceaccount.com'\n",
    "print(f'Service Account: {SERVICE_ACCOUNT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "29be00c3-756d-42f2-96ad-ab5fb39aac78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://mpg3-testflights-polished-vault-379315/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'mpg3-testflights-polished-vault-379315' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n",
      "                                 gs://mpg3-testflights-polished-vault-379315/data/\n",
      "                                 gs://mpg3-testflights-polished-vault-379315/pipeline-output/\n"
     ]
    }
   ],
   "source": [
    "# BUCKET_NAME = 'training_data_' + PROJECT_ID\n",
    "BUCKET_NAME = 'mpg3-testflights-polished-vault-379315'\n",
    "# BUCKET_NAME = 'mpg3-temp-data'\n",
    "\n",
    "BUCKET_URI = \"gs://\" + BUCKET_NAME\n",
    "! gsutil mb -l $REGION $BUCKET_URI\n",
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "790a3d31-1beb-4047-a96d-b72c7f44d7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 gs://mpg3-testflights-polished-vault-379315/data/\n",
      "                                 gs://mpg3-testflights-polished-vault-379315/pipeline-output/\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4d98a5c0-8403-4ce2-a57b-080824ef9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most of commands from setup.sh are still missing. need to translate bash code into python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "14885488-d726-4019-917c-7049c00f3ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpg3-testflights-polished-vault-379315\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform as aip\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact,\n",
    "    Dataset,\n",
    "    Input,\n",
    "    Model,\n",
    "    Output,\n",
    "    ClassificationMetrics,\n",
    "    component,\n",
    "    pipeline,\n",
    ")\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "\n",
    "\n",
    "# BUCKET = f\"training_data_{PROJECT_ID}\"\n",
    "BUCKET = BUCKET_NAME\n",
    "pipeline_root_path = f\"gs://{BUCKET}/pipeline-output/\"\n",
    "print(BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cb5d572d-e7f7-4c92-bf8b-7ca5b446074d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://mpg3-testflights-polished-vault-379315/pipeline-output/'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_root_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "438699c6-e837-413c-b227-e97124417bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=['gcsfs', 'fsspec'],\n",
    "    base_image=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\",\n",
    ")\n",
    "def data_download(\n",
    "    data_url: str,\n",
    "    split_date: str,\n",
    "    dataset_train: Output[Dataset],\n",
    "    dataset_test: Output[Dataset],\n",
    "):\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "\n",
    "    logging.warn(\"Import file:\", data_url)\n",
    "\n",
    "    data = pd.read_csv(data_url, nrows=5000)\n",
    "\n",
    "    cancelled = (data[\"Cancelled\"] > 0) | (data[\"Diverted\"] > 0)\n",
    "    completed_flights = data[~cancelled]\n",
    "\n",
    "    training_data = completed_flights[[\"DepDelay\", \"TaxiOut\", \"Distance\"]]\n",
    "    # Consider flights that arrive more than 15 min late as delayed\n",
    "    training_data[\"target\"] = completed_flights[\"ArrDelay\"] > 15\n",
    "\n",
    "    test_data = training_data[completed_flights[\"FlightDate\"] >= split_date]\n",
    "    training_data = training_data[completed_flights[\"FlightDate\"] < split_date]\n",
    "\n",
    "    training_data.to_csv(dataset_train.path, index=False)\n",
    "    test_data.to_csv(dataset_test.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9f077d25-f7de-4464-a647-a759aff1d772",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\",\n",
    ")\n",
    "def model_train(\n",
    "    dataset: Input[Dataset],\n",
    "    model: Output[Artifact],\n",
    "):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    data = pd.read_csv(dataset.path)\n",
    "    X = data.drop(columns=[\"target\"])\n",
    "    y = data[\"target\"]\n",
    "\n",
    "    model_pipeline = Pipeline(\n",
    "        [\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"clf\", LogisticRegression(random_state=42, tol=0.0001, max_iter=100)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model_pipeline.fit(X, y)\n",
    "\n",
    "    model.metadata[\"framework\"] = \"scikit-learn\"\n",
    "    model.metadata[\"containerSpec\"] = {\n",
    "        \"imageUri\": \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\"\n",
    "    }\n",
    "\n",
    "    file_name = model.path + \"/model.pkl\"\n",
    "    import pathlib\n",
    "\n",
    "    pathlib.Path(model.path).mkdir()\n",
    "    with open(file_name, \"wb\") as file:\n",
    "        pickle.dump(model_pipeline, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a82f9bd6-4cd8-43c7-bbf7-7758784a9c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\",\n",
    ")\n",
    "def model_evaluate(\n",
    "    test_set: Input[Dataset],\n",
    "    model: Input[Model],\n",
    "    metrics: Output[ClassificationMetrics],\n",
    "):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from sklearn.metrics import roc_curve, confusion_matrix, accuracy_score\n",
    "\n",
    "    data = pd.read_csv(test_set.path)[:1000]\n",
    "    file_name = model.path + \"/model.pkl\"\n",
    "    with open(file_name, \"rb\") as file:\n",
    "        model_pipeline = pickle.load(file)\n",
    "\n",
    "    X = data.drop(columns=[\"target\"])\n",
    "    y = data.target\n",
    "    y_pred = model_pipeline.predict(X)\n",
    "\n",
    "    y_scores = model_pipeline.predict_proba(X)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_true=y, y_score=y_scores, pos_label=True)\n",
    "    metrics.log_roc_curve(fpr.tolist(), tpr.tolist(), thresholds.tolist())\n",
    "\n",
    "    metrics.log_confusion_matrix(\n",
    "        [\"False\", \"True\"],\n",
    "        confusion_matrix(y, y_pred).tolist(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ec1d89d5-4eef-41b5-a222-87ce6e8b12de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the workflow of the pipeline.\n",
    "@pipeline(name=\"gcp-mlops-v0\", pipeline_root=pipeline_root_path)\n",
    "def pipeline(\n",
    "    training_data_url: str = f\"gs://{BUCKET}/data/processed/2020/2020-05.csv\",\n",
    "    test_split_date: str = \"2020-05-20\",\n",
    "):\n",
    "    data_op = data_download(\n",
    "        data_url=training_data_url,\n",
    "        split_date=test_split_date\n",
    "    )\n",
    "\n",
    "    from google_cloud_pipeline_components.experimental.custom_job.utils import (\n",
    "        create_custom_training_job_op_from_component,\n",
    "    )\n",
    "\n",
    "    custom_job_distributed_training_op = create_custom_training_job_op_from_component(\n",
    "        model_train, \n",
    "        replica_count=1, \n",
    "        machine_type = 'n1-standard-8'\n",
    "    )\n",
    "\n",
    "    model_train_op = custom_job_distributed_training_op(\n",
    "        dataset=data_op.outputs[\"dataset_train\"],\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "    )\n",
    "\n",
    "    model_evaluate_op = model_evaluate(\n",
    "        test_set=data_op.outputs[\"dataset_test\"],\n",
    "        model=model_train_op.outputs[\"model\"],\n",
    "    )\n",
    "\n",
    "    model_upload_op = ModelUploadOp(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        display_name=\"flight-delay-model\",\n",
    "        unmanaged_container_model=model_train_op.outputs[\"model\"],\n",
    "    ).after(model_evaluate_op)\n",
    "\n",
    "    endpoint_create_op = EndpointCreateOp(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        display_name=\"flight-delay-endpoint12\",\n",
    "    )\n",
    "\n",
    "    ModelDeployOp(\n",
    "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "        model=model_upload_op.outputs[\"model\"],\n",
    "        deployed_model_display_name=\"flight-delay-model\",\n",
    "        dedicated_resources_machine_type=\"n1-standard-8\",\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=3,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7b4f3496-8d3e-4525-834d-e9904375480f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/662390005506/locations/us-central1/pipelineJobs/gcp-mlops-v0-20230423181825\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/662390005506/locations/us-central1/pipelineJobs/gcp-mlops-v0-20230423181825')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/gcp-mlops-v0-20230423181825?project=662390005506\n",
      "PipelineJob projects/662390005506/locations/us-central1/pipelineJobs/gcp-mlops-v0-20230423181825 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/662390005506/locations/us-central1/pipelineJobs/gcp-mlops-v0-20230423181825 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/662390005506/locations/us-central1/pipelineJobs/gcp-mlops-v0-20230423181825 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/662390005506/locations/us-central1/pipelineJobs/gcp-mlops-v0-20230423181825 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/662390005506/locations/us-central1/pipelineJobs/gcp-mlops-v0-20230423181825 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/662390005506/locations/us-central1/pipelineJobs/gcp-mlops-v0-20230423181825\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"gcp-mlops-v0.json\")\n",
    "\n",
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET, location=REGION)\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"gcp-mlops-v0\",\n",
    "    template_path=\"gcp-mlops-v0.json\",\n",
    "    pipeline_root=pipeline_root_path,\n",
    ")\n",
    "\n",
    "job.run(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e3cdc3-0984-494d-9341-3fba164f5c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c49c00-c81a-4d9c-b707-435b1bc95af3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabc1fcc-044e-411c-a7fa-87052e63f77f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3ed8b8-f1cf-4b7f-b2ec-3b24e18a5692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee800e14-9e8f-4d61-a7d4-396aac192ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11b10e5-437e-46fc-9cfc-5da836695e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae2357c-93fe-4c52-8ada-548e53036982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fe7915ef-b490-4eee-bc94-4a46b9ae3a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is: Prediction(predictions=[False], deployed_model_id='2932338137650692096', model_version_id='1', model_resource_name='projects/662390005506/locations/us-central1/models/4506214266021347328', explanations=None)\n"
     ]
    }
   ],
   "source": [
    "# predictions from Python\n",
    "\n",
    "ENDPOINT_ID = '2557670754392997888'\n",
    "# get it from gcloud ai endpoints list. gcloud config set project polished-vault-379315. \n",
    "\n",
    "from google.cloud import aiplatform as aip\n",
    "\n",
    "aip.init(project=PROJECT_ID, location=REGION)\n",
    "endpoint = aip.Endpoint(ENDPOINT_ID)\n",
    "prediction = endpoint.predict(instances=[[-4.0, 16.0, 153.0]])\n",
    "print(f'Prediction is: {prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f181314-5165-408c-9e4f-0b04eec1bfaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3469231046.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/tmp/ipykernel_463/3469231046.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    nano INPUT.json\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# use the code below in Shell to test the endpoint.\n",
    "\n",
    "# gcloud auth application-default login\n",
    "nano INPUT.json\n",
    "\n",
    "{\n",
    "  \"instances\": [{1, 15, 400}]\n",
    "}\n",
    "\n",
    "ENDPOINT_ID=\"3891580669024796672\"\n",
    "PROJECT_ID=\"polished-vault-379315\"\n",
    "INPUT_DATA_FILE=\"INPUT.json\"\n",
    "\n",
    "curl \\\n",
    "-X POST \\\n",
    "-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-central1/endpoints/${ENDPOINT_ID}:predict \\\n",
    "-d \"@${INPUT_DATA_FILE}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bdd010-0b5b-4a30-8eab-971b34313769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86748e80-1049-4f84-936e-8406c382376f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be54b8c0-083b-4ec0-b8c8-9e2cc76ae54d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5411266c-7dc0-4551-a659-cba61498b273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65676f5a-2ed3-4b3b-9ff9-c45e1c26cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_url: str = f\"gs://{BUCKET}/data/processed/2020/2020-05.csv\"\n",
    "training_data_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aa88dc-d320-4438-bfc9-4120f19b65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(training_data_url, nrows=2000)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acccd9c3-afc1-411f-b2c6-b9f9b51ce7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data[[\"DepDelay\", \"TaxiOut\", \"Distance\"]]\n",
    "training_data.head()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "py38",
   "name": "common-cpu.m107",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m107"
  },
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
