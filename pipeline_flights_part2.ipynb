{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64befc0f-ee9f-42ef-b435-b854bf98ed86",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Simple example of Kubeflow pipeline to predict flight delays\n",
    "Code is borrowed from https://aiinpractice.com/gcp-mlops-vertex-ai-pipeline-scikit-learn/ and\n",
    "https://aiinpractice.com/gcp-mlops-vertex-ai-feature-store/\n",
    "\n",
    "I takes around 10 min to run the pipeline. At the end of the notebook see how to use prediction endpoint.\n",
    "\n",
    "\n",
    "Project works, but there are weird bugs with the bucket and data file. For some reason, only mpg3-temp-data works for now. \n",
    "\n",
    "#### Notes:\n",
    "- Project works, but there are weird bugs with the bucket and data file. For some reason, only mpg3-temp-data works for now. \n",
    "- There are often bugs when trying to create an endpoint with the same name as previously created and deleted endpoint in the same region.\n",
    "- The slowest part of the pipeline is deploying model to an endpoint. Using more powerful instance for an endpoint seems to speed up this step. after standard-8 more powerful insances seem to deploy slower. replica=3 seems to help too. Surprisingly, increasing replica count further makes model deployment slower. \n",
    "\n",
    "#### Next steps:\n",
    "1. Figure out how to use any bucket and any data file. Done\n",
    "2. Use more powerful instances to speed up all steps. Done, does not help much\n",
    "3. Go to the next part and add preprocessing pipeline.\n",
    "4. How to productionalize this pipeline?\n",
    "- (i) write daily cron job to try to pull new monthly data. when it succeeds, trigger this pipeline to retrain the model and save new nobthly perf-eval artifact.\n",
    "- (ii) simulate real-time user request daily. Use this to record daily perf-eval results dashboard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eef972-f743-4ad6-b013-7fe7d75e5f2f",
   "metadata": {},
   "source": [
    "#### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "058f9587-16fc-407c-afaf-f388f3666c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install Python 3.8 kernel into py38 environment\n",
    "# !bash startup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fba76421-fc1b-4d93-b17b-8820d332c1bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.16'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from platform import python_version\n",
    "python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a622e2a-f0b0-45a6-9790-5f3ab987518e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Featurestore\n",
      "Create Featurestore backing LRO: projects/662390005506/locations/us-central1/featurestores/flight_delays1/operations/9211118873116409856\n",
      "Featurestore created. Resource name: projects/662390005506/locations/us-central1/featurestores/flight_delays1\n",
      "To use this Featurestore in another session:\n",
      "featurestore = aiplatform.Featurestore('projects/662390005506/locations/us-central1/featurestores/flight_delays1')\n",
      "Creating EntityType\n",
      "Create EntityType backing LRO: projects/662390005506/locations/us-central1/featurestores/flight_delays1/entityTypes/flight/operations/7159729242849148928\n",
      "EntityType created. Resource name: projects/662390005506/locations/us-central1/featurestores/flight_delays1/entityTypes/flight\n",
      "To use this EntityType in another session:\n",
      "entity_type = aiplatform.EntityType('projects/662390005506/locations/us-central1/featurestores/flight_delays1/entityTypes/flight')\n",
      "Batch creating features EntityType entityType: projects/662390005506/locations/us-central1/featurestores/flight_delays1/entityTypes/flight\n",
      "Batch create Features EntityType entityType backing LRO: projects/662390005506/locations/us-central1/featurestores/flight_delays1/entityTypes/flight/operations/5205167004570353664\n",
      "EntityType entityType Batch created features. Resource name: projects/662390005506/locations/us-central1/featurestores/flight_delays1/entityTypes/flight\n",
      "Creating EntityType\n",
      "Create EntityType backing LRO: projects/662390005506/locations/us-central1/featurestores/flight_delays1/entityTypes/airport/operations/2899323995356659712\n",
      "EntityType created. Resource name: projects/662390005506/locations/us-central1/featurestores/flight_delays1/entityTypes/airport\n",
      "To use this EntityType in another session:\n",
      "entity_type = aiplatform.EntityType('projects/662390005506/locations/us-central1/featurestores/flight_delays1/entityTypes/airport')\n",
      "Creating Feature\n",
      "Create Feature backing LRO: projects/662390005506/locations/us-central1/featurestores/flight_delays1/entityTypes/airport/features/average_departure_delay/operations/6610290088309948416\n",
      "Feature created. Resource name: projects/662390005506/locations/us-central1/featurestores/flight_delays1/entityTypes/airport/features/average_departure_delay\n",
      "To use this Feature in another session:\n",
      "feature = aiplatform.Feature('projects/662390005506/locations/us-central1/featurestores/flight_delays1/entityTypes/airport/features/average_departure_delay')\n",
      "Time to create FeatureStore: 361.46 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from google.cloud import aiplatform as aip\n",
    "PROJECT_ID = \"polished-vault-379315\"  \n",
    "REGION = \"us-central1\"\n",
    "BUCKET = 'mpg3-testflights-polished-vault-379315'\n",
    "\n",
    "time0 = time.time()\n",
    "\n",
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET, location=REGION)\n",
    "\n",
    "flight_delays_feature_store = aip.Featurestore.create(\n",
    "    \"flight_delays1\", online_store_fixed_node_count=1\n",
    ")\n",
    "\n",
    "flight_entity_type = flight_delays_feature_store.create_entity_type(\n",
    "    entity_type_id=\"flight\",\n",
    "    description=\"Flight entity\",\n",
    ")\n",
    "\n",
    "flight_entity_type.batch_create_features(\n",
    "    {\n",
    "        \"origin_airport_id\": {\n",
    "            \"value_type\": \"STRING\",\n",
    "            \"description\": \"Airport ID for the origin\",\n",
    "        },\n",
    "        \"is_cancelled\": {\n",
    "            \"value_type\": \"BOOL\",\n",
    "            \"description\": \"Has the flight been cancelled or diverted?\",\n",
    "        },\n",
    "        \"departure_delay_minutes\": {\n",
    "            \"value_type\": \"DOUBLE\",\n",
    "            \"description\": \"Departure delay in minutes\",\n",
    "        },\n",
    "        \"arrival_delay_minutes\": {\n",
    "            \"value_type\": \"DOUBLE\",\n",
    "            \"description\": \"Arrival delay in minutes\",\n",
    "        },\n",
    "        \"taxi_out_minutes\": {\n",
    "            \"value_type\": \"DOUBLE\",\n",
    "            \"description\": \"Taxi out time in minutes\",\n",
    "        },\n",
    "        \"distance_miles\": {\n",
    "            \"value_type\": \"DOUBLE\",\n",
    "            \"description\": \"Total flight distance in miles.\",\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "airport_entity_type = flight_delays_feature_store.create_entity_type(\n",
    "    entity_type_id=\"airport\",\n",
    "    description=\"Airport entity\",\n",
    ")\n",
    "\n",
    "airport_entity_type.create_feature(\n",
    "    feature_id=\"average_departure_delay\",\n",
    "    value_type=\"DOUBLE\",\n",
    "    description=\"Average departure delay for that airport, calculated every 4h with 1h rolling window\",\n",
    ")\n",
    "\n",
    "print(f'Time to create FeatureStore: {time.time()-time0:.2f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d41dce0f-967c-4ff7-9cba-0e80fa1a7c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get featurestore_id from the above output. \n",
    "# if forgot, it may be easier to recreate fs with a new name.\n",
    "FEATURE_STORE_ID = \"662390005506\"\n",
    "# ENDPOINT_ID = \"xxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1726ac82-bcc1-41f1-abb5-76d9065dda8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_pipeline/feature_pipeline/helpers.py\n",
    "\n",
    "from typing import Union, get_args, get_origin\n",
    "from datetime import datetime\n",
    "\n",
    "def map_to_avro_type(field_type):\n",
    "    if field_type == str:\n",
    "        return \"string\"\n",
    "    elif field_type == bool:\n",
    "        return \"boolean\"\n",
    "    elif field_type == float:\n",
    "        return \"double\"\n",
    "    elif field_type is type(None):\n",
    "        return \"null\"\n",
    "    elif field_type == datetime:\n",
    "        return {\"type\": \"long\", \"logicalType\": \"timestamp-micros\"}\n",
    "    elif get_origin(field_type) == Union:\n",
    "        return [map_to_avro_type(t) for t in get_args(field_type)]\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unsupported type: {field_type}\")\n",
    "\n",
    "\n",
    "def named_tuple_to_avro_fields(named_tuple):\n",
    "    fields = []\n",
    "    for field_name, field_type in named_tuple.__annotations__.items():\n",
    "        fields.append({\"name\": field_name, \"type\": map_to_avro_type(field_type)})\n",
    "    return fields\n",
    "\n",
    "\n",
    "csv_headers = [\n",
    "    \"Year\",\n",
    "    \"Quarter\",\n",
    "    \"Month\",\n",
    "    \"DayofMonth\",\n",
    "    \"DayOfWeek\",\n",
    "    \"FlightDate\",\n",
    "    \"Reporting_Airline\",\n",
    "    \"DOT_ID_Reporting_Airline\",\n",
    "    \"IATA_CODE_Reporting_Airline\",\n",
    "    \"Tail_Number\",\n",
    "    \"Flight_Number_Reporting_Airline\",\n",
    "    \"OriginAirportID\",\n",
    "    \"OriginAirportSeqID\",\n",
    "    \"OriginCityMarketID\",\n",
    "    \"Origin\",\n",
    "    \"OriginCityName\",\n",
    "    \"OriginState\",\n",
    "    \"OriginStateFips\",\n",
    "    \"OriginStateName\",\n",
    "    \"OriginWac\",\n",
    "    \"DestAirportID\",\n",
    "    \"DestAirportSeqID\",\n",
    "    \"DestCityMarketID\",\n",
    "    \"Dest\",\n",
    "    \"DestCityName\",\n",
    "    \"DestState\",\n",
    "    \"DestStateFips\",\n",
    "    \"DestStateName\",\n",
    "    \"DestWac\",\n",
    "    \"CRSDepTime\",\n",
    "    \"DepTime\",\n",
    "    \"DepDelay\",\n",
    "    \"DepDelayMinutes\",\n",
    "    \"DepDel15\",\n",
    "    \"DepartureDelayGroups\",\n",
    "    \"DepTimeBlk\",\n",
    "    \"TaxiOut\",\n",
    "    \"WheelsOff\",\n",
    "    \"WheelsOn\",\n",
    "    \"TaxiIn\",\n",
    "    \"CRSArrTime\",\n",
    "    \"ArrTime\",\n",
    "    \"ArrDelay\",\n",
    "    \"ArrDelayMinutes\",\n",
    "    \"ArrDel15\",\n",
    "    \"ArrivalDelayGroups\",\n",
    "    \"ArrTimeBlk\",\n",
    "    \"Cancelled\",\n",
    "    \"CancellationCode\",\n",
    "    \"Diverted\",\n",
    "    \"CRSElapsedTime\",\n",
    "    \"ActualElapsedTime\",\n",
    "    \"AirTime\",\n",
    "    \"Flights\",\n",
    "    \"Distance\",\n",
    "    \"DistanceGroup\",\n",
    "    \"CarrierDelay\",\n",
    "    \"WeatherDelay\",\n",
    "    \"NASDelay\",\n",
    "    \"SecurityDelay\",\n",
    "    \"LateAircraftDelay\",\n",
    "    \"FirstDepTime\",\n",
    "    \"TotalAddGTime\",\n",
    "    \"LongestAddGTime\",\n",
    "    \"DivAirportLandings\",\n",
    "    \"DivReachedDest\",\n",
    "    \"DivActualElapsedTime\",\n",
    "    \"DivArrDelay\",\n",
    "    \"DivDistance\",\n",
    "    \"Div1Airport\",\n",
    "    \"Div1AirportID\",\n",
    "    \"Div1AirportSeqID\",\n",
    "    \"Div1WheelsOn\",\n",
    "    \"Div1TotalGTime\",\n",
    "    \"Div1LongestGTime\",\n",
    "    \"Div1WheelsOff\",\n",
    "    \"Div1TailNum\",\n",
    "    \"Div2Airport\",\n",
    "    \"Div2AirportID\",\n",
    "    \"Div2AirportSeqID\",\n",
    "    \"Div2WheelsOn\",\n",
    "    \"Div2TotalGTime\",\n",
    "    \"Div2LongestGTime\",\n",
    "    \"Div2WheelsOff\",\n",
    "    \"Div2TailNum\",\n",
    "    \"Div3Airport\",\n",
    "    \"Div3AirportID\",\n",
    "    \"Div3AirportSeqID\",\n",
    "    \"Div3WheelsOn\",\n",
    "    \"Div3TotalGTime\",\n",
    "    \"Div3LongestGTime\",\n",
    "    \"Div3WheelsOff\",\n",
    "    \"Div3TailNum\",\n",
    "    \"Div4Airport\",\n",
    "    \"Div4AirportID\",\n",
    "    \"Div4AirportSeqID\",\n",
    "    \"Div4WheelsOn\",\n",
    "    \"Div4TotalGTime\",\n",
    "    \"Div4LongestGTime\",\n",
    "    \"Div4WheelsOff\",\n",
    "    \"Div4TailNum\",\n",
    "    \"Div5Airport\",\n",
    "    \"Div5AirportID\",\n",
    "    \"Div5AirportSeqID\",\n",
    "    \"Div5WheelsOn\",\n",
    "    \"Div5TotalGTime\",\n",
    "    \"Div5LongestGTime\",\n",
    "    \"Div5WheelsOff\",\n",
    "    \"Div5TailNum\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c1b81af-0279-4c3c-b236-8be0234df233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "class Flight(NamedTuple):\n",
    "    timestamp: Optional[datetime]\n",
    "    flight_number: str\n",
    "    origin_airport_id: str\n",
    "    is_cancelled: bool\n",
    "    departure_delay_minutes: float\n",
    "    arrival_delay_minutes: float\n",
    "    taxi_out_minutes: float\n",
    "    distance_miles: float\n",
    "\n",
    "\n",
    "flight_avro_schema = {\n",
    "    \"namespace\": \"flight_delay_prediction\",\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Flight\",\n",
    "    \"fields\": named_tuple_to_avro_fields(Flight),\n",
    "}\n",
    "\n",
    "\n",
    "class AirportFeatures(NamedTuple):\n",
    "    timestamp: Optional[datetime]\n",
    "    origin_airport_id: str\n",
    "    average_departure_delay: float\n",
    "\n",
    "\n",
    "airport_avro_schema = {\n",
    "    \"namespace\": \"flight_delay_prediction\",\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Airport\",\n",
    "    \"fields\": named_tuple_to_avro_fields(AirportFeatures),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d67a5b-3d35-4d4c-98d6-aeb3607e8c6e",
   "metadata": {},
   "source": [
    "#### 2.Build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c0c0fe6-5a1a-410f-84a4-f64a178618a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_feature_pipeline.py\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "\n",
    "\n",
    "def parse_csv(line: str):\n",
    "    import csv\n",
    "    return next(csv.reader([line]))\n",
    "\n",
    "\n",
    "def parse_line(fields):\n",
    "    from datetime import datetime\n",
    "    from apache_beam.utils.timestamp import Timestamp\n",
    "\n",
    "    data = dict(zip(csv_headers, fields))\n",
    "\n",
    "    if (\n",
    "        data[\"Year\"] != \"Year\"  # skip header row\n",
    "        and len(data[\"WheelsOff\"]) == 4  #\n",
    "        and len(data[\"FlightDate\"]) == 10  # row has a flight date\n",
    "        and data[\"Distance\"] != \"\"\n",
    "    ):\n",
    "        wheels_off_hour = data[\"WheelsOff\"][:2]\n",
    "        wheels_off_minutes = data[\"WheelsOff\"][2:]\n",
    "        departure_date_time = (\n",
    "            f\"{data['FlightDate']}T{wheels_off_hour}:{wheels_off_minutes}:00\"\n",
    "        )\n",
    "\n",
    "        cancelled = (float(data[\"Cancelled\"]) > 0) or (float(data[\"Diverted\"]) > 0)\n",
    "\n",
    "        try:\n",
    "            flight = Flight(\n",
    "                timestamp=datetime.fromisoformat(departure_date_time),\n",
    "                origin_airport_id=str(data[\"OriginAirportID\"]),\n",
    "                flight_number=f\"{data['Reporting_Airline']}//{data['Flight_Number_Reporting_Airline']}\",\n",
    "                is_cancelled=cancelled,\n",
    "                departure_delay_minutes=float(data[\"DepDelay\"]),\n",
    "                arrival_delay_minutes=float(data[\"ArrDelay\"]),\n",
    "                taxi_out_minutes=float(data[\"TaxiOut\"]),\n",
    "                distance_miles=float(data[\"Distance\"]),\n",
    "            )\n",
    "\n",
    "            yield beam.window.TimestampedValue(\n",
    "                flight, Timestamp.from_rfc3339(departure_date_time)\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "class BuildTimestampedRecordFn(beam.DoFn):\n",
    "    def process(self, element, window=beam.DoFn.WindowParam):\n",
    "\n",
    "        window_start = window.start.to_utc_datetime()\n",
    "        return [\n",
    "            AirportFeatures(\n",
    "                timestamp=window_start,\n",
    "                origin_airport_id=element.origin_airport_id,\n",
    "                average_departure_delay=element.average_departure_delay,\n",
    "            )._asdict()\n",
    "        ]\n",
    "\n",
    "\n",
    "class BuildTimestampedFlightRecordFn(beam.DoFn):\n",
    "    def process(self, element: Flight, window=beam.DoFn.WindowParam):\n",
    "        return [element._asdict()]\n",
    "\n",
    "\n",
    "def run(argv=None, save_main_session=False):\n",
    "    \"\"\"Main entry point; defines and runs the wordcount pipeline.\n",
    "    never mind default arguments, they will not be invoked.\"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--input\",\n",
    "        dest=\"input\",\n",
    "        default=\"/Users/simon/projects/private/gcp_mlops/data/processed/2020/2020-05.csv\",\n",
    "        help=\"Input file to process.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output-airports\",\n",
    "        dest=\"output_airports\",\n",
    "        default=\"/Users/simon/projects/private/gcp_mlops/data/output_airports/\",\n",
    "        help=\"Output file to write results to.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--output-flights\",\n",
    "        dest=\"output_flights\",\n",
    "        default=\"/Users/simon/projects/private/gcp_mlops/data/output_flights/\",\n",
    "        help=\"Output file to write results to.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--output-read-instances\",\n",
    "        dest=\"output_read_instances\",\n",
    "        default=\"/Users/simon/projects/private/gcp_mlops/data/output_read_instances/\",\n",
    "        help=\"Output file to write results to.\",\n",
    "    )\n",
    "\n",
    "    # Parse beam arguments (e.g. --runner=DirectRunner to run the pipeline locally)\n",
    "    known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "\n",
    "    # We use the save_main_session option because one or more DoFn's in this\n",
    "    # workflow rely on global context (e.g., a module imported at module level).\n",
    "    pipeline_options = PipelineOptions(pipeline_args)\n",
    "    pipeline_options.view_as(SetupOptions).save_main_session = save_main_session\n",
    "\n",
    "    with beam.Pipeline(options=pipeline_options) as pipeline:\n",
    "        flights = (\n",
    "            pipeline\n",
    "            | \"read_input\" >> beam.io.ReadFromText(known_args.input)\n",
    "            | \"parse_csv\" >> beam.Map(parse_csv)\n",
    "            | \"create_flight_obj\" >> beam.FlatMap(parse_line).with_output_types(Flight)\n",
    "        )\n",
    "\n",
    "        # Create airport data\n",
    "        (\n",
    "            flights\n",
    "            | \"window\"\n",
    "            >> beam.WindowInto(\n",
    "                beam.window.SlidingWindows(4 * 60 * 60, 60 * 60)\n",
    "            )  # 4h time windows, every 60min\n",
    "            | \"group_by_airport\"\n",
    "            >> beam.GroupBy(\"origin_airport_id\").aggregate_field(\n",
    "                \"departure_delay_minutes\",\n",
    "                beam.combiners.MeanCombineFn(),\n",
    "                \"average_departure_delay\",\n",
    "            )\n",
    "            | \"add_timestamp\" >> beam.ParDo(BuildTimestampedRecordFn())\n",
    "            | \"write_airport_data\"\n",
    "            >> beam.io.WriteToAvro(\n",
    "                known_args.output_airports, schema=airport_avro_schema\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Create flight data\n",
    "        (\n",
    "            flights\n",
    "            | \"format_output\" >> beam.ParDo(BuildTimestampedFlightRecordFn())\n",
    "            | \"write_flight_data\"\n",
    "            >> beam.io.WriteToAvro(known_args.output_flights, schema=flight_avro_schema)\n",
    "        )\n",
    "\n",
    "        # Create read_instances.csv to retrieve training data from the feature store\n",
    "        (\n",
    "            flights\n",
    "            | \"format_read_instances_output\"\n",
    "            >> beam.Map(\n",
    "                lambda flight: f\"{flight.flight_number},{flight.origin_airport_id},{flight.timestamp.isoformat('T') + 'Z'}\"\n",
    "            )\n",
    "            | \"write_read_instances\"\n",
    "            >> beam.io.WriteToText(\n",
    "                known_args.output_read_instances,\n",
    "                file_name_suffix=\".csv\",\n",
    "                num_shards=1,\n",
    "                header=\"flight,airport,timestamp\",\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d5fbb5-8fc7-4bc0-9ec7-597db3b63315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this pipeline\n",
    "\n",
    "!bash startup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddead7c9-9ab4-4e70-ad90-792094314ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462c17b9-4504-477c-a46d-a8c60d6f4f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1c9cc1-9a12-4c16-a933-0938b505b59a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bddf2e8-5bc9-4aac-829d-5fecd4151532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingest batch features:\n",
    "\n",
    "from google.cloud import aiplatform as aip\n",
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET, location=REGION)\n",
    "\n",
    "flight_delays_feature_store = aip.Featurestore(\n",
    "    FEATURE_STORE_ID,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")\n",
    "\n",
    "flight_entity_type = flight_delays_feature_store.get_entity_type(\"flight\")\n",
    "flight_entity_type.ingest_from_gcs(\n",
    "    feature_ids=[\n",
    "        \"origin_airport_id\",\n",
    "        \"is_cancelled\",\n",
    "        \"departure_delay_minutes\",\n",
    "        \"arrival_delay_minutes\",\n",
    "        \"taxi_out_minutes\",\n",
    "        \"distance_miles\",\n",
    "    ],\n",
    "    feature_time=\"timestamp\",\n",
    "    gcs_source_uris=f\"gs://{BUCKET}/features/flight_features/*\",\n",
    "    gcs_source_type=\"avro\",\n",
    "    entity_id_field=\"flight_number\",\n",
    ")\n",
    "\n",
    "airport_entity_type = flight_delays_feature_store.get_entity_type(\"airport\")\n",
    "airport_entity_type.ingest_from_gcs(\n",
    "    feature_ids=[\"average_departure_delay\"],\n",
    "    feature_time=\"timestamp\",\n",
    "    gcs_source_uris=f\"gs://{BUCKET}/features/airport_features/*\",\n",
    "    gcs_source_type=\"avro\",\n",
    "    entity_id_field=\"origin_airport_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f058cbb8-392a-4d46-9989-344560f45fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9333d6-bda9-43f6-a524-e6e83ba65836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c297587f-463c-421f-bc61-b4246271c5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2135ab10-49df-4da6-8d46-bc437414df5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79ee811-9b3b-4ad7-a34e-0d580c8f4264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b541e91-efc0-49d1-b479-130d88b5dbc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd45b9b-2db9-4f78-a776-640553206293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e20e3e5-4445-4019-8afe-01c414dd80f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bcef887-b1bb-4db4-bf88-07874ad61b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next step should be to run pipeline. in his repo, this is main.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd1d4b3-da69-4fca-b666-77cdc4dc77b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a02380-5d56-49fc-ad5d-6e9b51965509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667284bd-4f66-4de0-8eae-a52742fb8d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fe7af7-fcb8-41ca-a616-70baddda0b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "43e6922e-18a5-4247-92c1-fc9cacca2e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "USER_NAME=\"oo00011760@gmail.com\" \n",
    "PROJECT_ID = \"polished-vault-379315\"  \n",
    "REGION = \"us-central1\"\n",
    "# REGION = \"us-east1\"\n",
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c0864806-79c7-49fe-8bb8-dacf3c263433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Account: vertex-ai-service-account@polished-vault-379315.iam.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "SERVICE_ACCOUNT = 'vertex-ai-service-account@polished-vault-379315.iam.gserviceaccount.com'\n",
    "print(f'Service Account: {SERVICE_ACCOUNT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "29be00c3-756d-42f2-96ad-ab5fb39aac78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://mpg3-testflights-polished-vault-379315/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'mpg3-testflights-polished-vault-379315' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n",
      "                                 gs://mpg3-testflights-polished-vault-379315/data/\n",
      "                                 gs://mpg3-testflights-polished-vault-379315/pipeline-output/\n"
     ]
    }
   ],
   "source": [
    "# BUCKET_NAME = 'training_data_' + PROJECT_ID\n",
    "BUCKET_NAME = 'mpg3-testflights-polished-vault-379315'\n",
    "# BUCKET_NAME = 'mpg3-temp-data'\n",
    "\n",
    "BUCKET_URI = \"gs://\" + BUCKET_NAME\n",
    "! gsutil mb -l $REGION $BUCKET_URI\n",
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "790a3d31-1beb-4047-a96d-b72c7f44d7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 gs://mpg3-testflights-polished-vault-379315/data/\n",
      "                                 gs://mpg3-testflights-polished-vault-379315/pipeline-output/\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4d98a5c0-8403-4ce2-a57b-080824ef9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most of commands from setup.sh are still missing. need to translate bash code into python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "14885488-d726-4019-917c-7049c00f3ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpg3-testflights-polished-vault-379315\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform as aip\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact,\n",
    "    Dataset,\n",
    "    Input,\n",
    "    Model,\n",
    "    Output,\n",
    "    ClassificationMetrics,\n",
    "    component,\n",
    "    pipeline,\n",
    ")\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "\n",
    "\n",
    "# BUCKET = f\"training_data_{PROJECT_ID}\"\n",
    "BUCKET = BUCKET_NAME\n",
    "pipeline_root_path = f\"gs://{BUCKET}/pipeline-output/\"\n",
    "print(BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cb5d572d-e7f7-4c92-bf8b-7ca5b446074d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://mpg3-testflights-polished-vault-379315/pipeline-output/'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_root_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "438699c6-e837-413c-b227-e97124417bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=['gcsfs', 'fsspec'],\n",
    "    base_image=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\",\n",
    ")\n",
    "def data_download(\n",
    "    data_url: str,\n",
    "    split_date: str,\n",
    "    dataset_train: Output[Dataset],\n",
    "    dataset_test: Output[Dataset],\n",
    "):\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "\n",
    "    logging.warn(\"Import file:\", data_url)\n",
    "\n",
    "    data = pd.read_csv(data_url, nrows=5000)\n",
    "\n",
    "    cancelled = (data[\"Cancelled\"] > 0) | (data[\"Diverted\"] > 0)\n",
    "    completed_flights = data[~cancelled]\n",
    "\n",
    "    training_data = completed_flights[[\"DepDelay\", \"TaxiOut\", \"Distance\"]]\n",
    "    # Consider flights that arrive more than 15 min late as delayed\n",
    "    training_data[\"target\"] = completed_flights[\"ArrDelay\"] > 15\n",
    "\n",
    "    test_data = training_data[completed_flights[\"FlightDate\"] >= split_date]\n",
    "    training_data = training_data[completed_flights[\"FlightDate\"] < split_date]\n",
    "\n",
    "    training_data.to_csv(dataset_train.path, index=False)\n",
    "    test_data.to_csv(dataset_test.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9f077d25-f7de-4464-a647-a759aff1d772",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\",\n",
    ")\n",
    "def model_train(\n",
    "    dataset: Input[Dataset],\n",
    "    model: Output[Artifact],\n",
    "):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    data = pd.read_csv(dataset.path)\n",
    "    X = data.drop(columns=[\"target\"])\n",
    "    y = data[\"target\"]\n",
    "\n",
    "    model_pipeline = Pipeline(\n",
    "        [\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"clf\", LogisticRegression(random_state=42, tol=0.0001, max_iter=100)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model_pipeline.fit(X, y)\n",
    "\n",
    "    model.metadata[\"framework\"] = \"scikit-learn\"\n",
    "    model.metadata[\"containerSpec\"] = {\n",
    "        \"imageUri\": \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\"\n",
    "    }\n",
    "\n",
    "    file_name = model.path + \"/model.pkl\"\n",
    "    import pathlib\n",
    "\n",
    "    pathlib.Path(model.path).mkdir()\n",
    "    with open(file_name, \"wb\") as file:\n",
    "        pickle.dump(model_pipeline, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a82f9bd6-4cd8-43c7-bbf7-7758784a9c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\",\n",
    ")\n",
    "def model_evaluate(\n",
    "    test_set: Input[Dataset],\n",
    "    model: Input[Model],\n",
    "    metrics: Output[ClassificationMetrics],\n",
    "):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from sklearn.metrics import roc_curve, confusion_matrix, accuracy_score\n",
    "\n",
    "    data = pd.read_csv(test_set.path)[:1000]\n",
    "    file_name = model.path + \"/model.pkl\"\n",
    "    with open(file_name, \"rb\") as file:\n",
    "        model_pipeline = pickle.load(file)\n",
    "\n",
    "    X = data.drop(columns=[\"target\"])\n",
    "    y = data.target\n",
    "    y_pred = model_pipeline.predict(X)\n",
    "\n",
    "    y_scores = model_pipeline.predict_proba(X)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_true=y, y_score=y_scores, pos_label=True)\n",
    "    metrics.log_roc_curve(fpr.tolist(), tpr.tolist(), thresholds.tolist())\n",
    "\n",
    "    metrics.log_confusion_matrix(\n",
    "        [\"False\", \"True\"],\n",
    "        confusion_matrix(y, y_pred).tolist(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ec1d89d5-4eef-41b5-a222-87ce6e8b12de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the workflow of the pipeline.\n",
    "@pipeline(name=\"gcp-mlops-v0\", pipeline_root=pipeline_root_path)\n",
    "def pipeline(\n",
    "    training_data_url: str = f\"gs://{BUCKET}/data/processed/2020/2020-05.csv\",\n",
    "    test_split_date: str = \"2020-05-20\",\n",
    "):\n",
    "    data_op = data_download(\n",
    "        data_url=training_data_url,\n",
    "        split_date=test_split_date\n",
    "    )\n",
    "\n",
    "    from google_cloud_pipeline_components.experimental.custom_job.utils import (\n",
    "        create_custom_training_job_op_from_component,\n",
    "    )\n",
    "\n",
    "    custom_job_distributed_training_op = create_custom_training_job_op_from_component(\n",
    "        model_train, \n",
    "        replica_count=1, \n",
    "        machine_type = 'n1-standard-8'\n",
    "    )\n",
    "\n",
    "    model_train_op = custom_job_distributed_training_op(\n",
    "        dataset=data_op.outputs[\"dataset_train\"],\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "    )\n",
    "\n",
    "    model_evaluate_op = model_evaluate(\n",
    "        test_set=data_op.outputs[\"dataset_test\"],\n",
    "        model=model_train_op.outputs[\"model\"],\n",
    "    )\n",
    "\n",
    "    model_upload_op = ModelUploadOp(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        display_name=\"flight-delay-model\",\n",
    "        unmanaged_container_model=model_train_op.outputs[\"model\"],\n",
    "    ).after(model_evaluate_op)\n",
    "\n",
    "    endpoint_create_op = EndpointCreateOp(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        display_name=\"flight-delay-endpoint12\",\n",
    "    )\n",
    "\n",
    "    ModelDeployOp(\n",
    "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "        model=model_upload_op.outputs[\"model\"],\n",
    "        deployed_model_display_name=\"flight-delay-model\",\n",
    "        dedicated_resources_machine_type=\"n1-standard-8\",\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=3,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7b4f3496-8d3e-4525-834d-e9904375480f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/662390005506/locations/us-central1/pipelineJobs/gcp-mlops-v0-20230423181825\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/662390005506/locations/us-central1/pipelineJobs/gcp-mlops-v0-20230423181825')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/gcp-mlops-v0-20230423181825?project=662390005506\n",
      "PipelineJob projects/662390005506/locations/us-central1/pipelineJobs/gcp-mlops-v0-20230423181825 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/662390005506/locations/us-central1/pipelineJobs/gcp-mlops-v0-20230423181825 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/662390005506/locations/us-central1/pipelineJobs/gcp-mlops-v0-20230423181825 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/662390005506/locations/us-central1/pipelineJobs/gcp-mlops-v0-20230423181825 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/662390005506/locations/us-central1/pipelineJobs/gcp-mlops-v0-20230423181825 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/662390005506/locations/us-central1/pipelineJobs/gcp-mlops-v0-20230423181825\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"gcp-mlops-v0.json\")\n",
    "\n",
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET, location=REGION)\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"gcp-mlops-v0\",\n",
    "    template_path=\"gcp-mlops-v0.json\",\n",
    "    pipeline_root=pipeline_root_path,\n",
    ")\n",
    "\n",
    "job.run(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e3cdc3-0984-494d-9341-3fba164f5c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c49c00-c81a-4d9c-b707-435b1bc95af3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabc1fcc-044e-411c-a7fa-87052e63f77f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3ed8b8-f1cf-4b7f-b2ec-3b24e18a5692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee800e14-9e8f-4d61-a7d4-396aac192ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11b10e5-437e-46fc-9cfc-5da836695e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae2357c-93fe-4c52-8ada-548e53036982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fe7915ef-b490-4eee-bc94-4a46b9ae3a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is: Prediction(predictions=[False], deployed_model_id='2932338137650692096', model_version_id='1', model_resource_name='projects/662390005506/locations/us-central1/models/4506214266021347328', explanations=None)\n"
     ]
    }
   ],
   "source": [
    "# predictions from Python\n",
    "\n",
    "ENDPOINT_ID = '2557670754392997888'\n",
    "# get it from gcloud ai endpoints list. gcloud config set project polished-vault-379315. \n",
    "\n",
    "from google.cloud import aiplatform as aip\n",
    "\n",
    "aip.init(project=PROJECT_ID, location=REGION)\n",
    "endpoint = aip.Endpoint(ENDPOINT_ID)\n",
    "prediction = endpoint.predict(instances=[[-4.0, 16.0, 153.0]])\n",
    "print(f'Prediction is: {prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f181314-5165-408c-9e4f-0b04eec1bfaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3469231046.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/tmp/ipykernel_463/3469231046.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    nano INPUT.json\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# use the code below in Shell to test the endpoint.\n",
    "\n",
    "# gcloud auth application-default login\n",
    "nano INPUT.json\n",
    "\n",
    "{\n",
    "  \"instances\": [{1, 15, 400}]\n",
    "}\n",
    "\n",
    "ENDPOINT_ID=\"3891580669024796672\"\n",
    "PROJECT_ID=\"polished-vault-379315\"\n",
    "INPUT_DATA_FILE=\"INPUT.json\"\n",
    "\n",
    "curl \\\n",
    "-X POST \\\n",
    "-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-central1/endpoints/${ENDPOINT_ID}:predict \\\n",
    "-d \"@${INPUT_DATA_FILE}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bdd010-0b5b-4a30-8eab-971b34313769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86748e80-1049-4f84-936e-8406c382376f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be54b8c0-083b-4ec0-b8c8-9e2cc76ae54d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5411266c-7dc0-4551-a659-cba61498b273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65676f5a-2ed3-4b3b-9ff9-c45e1c26cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_url: str = f\"gs://{BUCKET}/data/processed/2020/2020-05.csv\"\n",
    "training_data_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aa88dc-d320-4438-bfc9-4120f19b65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(training_data_url, nrows=2000)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acccd9c3-afc1-411f-b2c6-b9f9b51ce7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data[[\"DepDelay\", \"TaxiOut\", \"Distance\"]]\n",
    "training_data.head()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "py38",
   "name": "common-cpu.m107",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m107"
  },
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
