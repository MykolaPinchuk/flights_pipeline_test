{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64befc0f-ee9f-42ef-b435-b854bf98ed86",
   "metadata": {},
   "source": [
    "## Simple example of Kubeflow pipeline to predict flight delays\n",
    "Code is borrowed from https://aiinpractice.com/gcp-mlops-vertex-ai-pipeline-scikit-learn/\n",
    "\n",
    "I takes around 10 min to run the pipeline. At the end of the notebook see how to use prediction endpoint.\n",
    "\n",
    "\n",
    "Project works, but there are weird bugs with the bucket and data file. For some reason, only mpg3-temp-data works for now. \n",
    "\n",
    "#### Notes:\n",
    "- Project works, but there are weird bugs with the bucket and data file. For some reason, only mpg3-temp-data works for now. \n",
    "- There are often bugs when trying to create an endpoint with the same name as previously created and deleted endpoint in the same region.\n",
    "- The slowest part of the pipeline is deploying model to an endpoint. Using more powerful instance for an endpoint seems to speed up this step. after standard-8 more powerful insances seem to deploy slower.\n",
    "\n",
    "#### Next steps:\n",
    "1. Figure out how to use any bucket and any data file. Done\n",
    "2. Use more powerful instances to speed up all steps.\n",
    "3. Go to the next part and add preprocessing pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "43e6922e-18a5-4247-92c1-fc9cacca2e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "USER_NAME=\"oo00011760@gmail.com\" \n",
    "PROJECT_ID = \"polished-vault-379315\"  \n",
    "REGION = \"us-central1\"\n",
    "# REGION = \"us-east1\"\n",
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c0864806-79c7-49fe-8bb8-dacf3c263433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Account: vertex-ai-service-account@polished-vault-379315.iam.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "SERVICE_ACCOUNT = 'vertex-ai-service-account@polished-vault-379315.iam.gserviceaccount.com'\n",
    "print(f'Service Account: {SERVICE_ACCOUNT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "29be00c3-756d-42f2-96ad-ab5fb39aac78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://mpg3-testflights-polished-vault-379315/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'mpg3-testflights-polished-vault-379315' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n",
      "                                 gs://mpg3-testflights-polished-vault-379315/data/\n",
      "                                 gs://mpg3-testflights-polished-vault-379315/pipeline-output/\n"
     ]
    }
   ],
   "source": [
    "# BUCKET_NAME = 'training_data_' + PROJECT_ID\n",
    "BUCKET_NAME = 'mpg3-testflights-polished-vault-379315'\n",
    "# BUCKET_NAME = 'mpg3-temp-data'\n",
    "\n",
    "BUCKET_URI = \"gs://\" + BUCKET_NAME\n",
    "! gsutil mb -l $REGION $BUCKET_URI\n",
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "790a3d31-1beb-4047-a96d-b72c7f44d7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 gs://mpg3-testflights-polished-vault-379315/data/\n",
      "                                 gs://mpg3-testflights-polished-vault-379315/pipeline-output/\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4d98a5c0-8403-4ce2-a57b-080824ef9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most of commands from setup.sh are still missing. need to translate bash code into python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "14885488-d726-4019-917c-7049c00f3ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpg3-testflights-polished-vault-379315\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform as aip\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact,\n",
    "    Dataset,\n",
    "    Input,\n",
    "    Model,\n",
    "    Output,\n",
    "    ClassificationMetrics,\n",
    "    component,\n",
    "    pipeline,\n",
    ")\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "\n",
    "\n",
    "# BUCKET = f\"training_data_{PROJECT_ID}\"\n",
    "BUCKET = BUCKET_NAME\n",
    "pipeline_root_path = f\"gs://{BUCKET}/pipeline-output/\"\n",
    "print(BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cb5d572d-e7f7-4c92-bf8b-7ca5b446074d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://mpg3-testflights-polished-vault-379315/pipeline-output/'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_root_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "438699c6-e837-413c-b227-e97124417bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=['gcsfs', 'fsspec'],\n",
    "    base_image=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\",\n",
    ")\n",
    "def data_download(\n",
    "    data_url: str,\n",
    "    split_date: str,\n",
    "    dataset_train: Output[Dataset],\n",
    "    dataset_test: Output[Dataset],\n",
    "):\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "\n",
    "    logging.warn(\"Import file:\", data_url)\n",
    "\n",
    "    data = pd.read_csv(data_url, nrows=5000)\n",
    "\n",
    "    cancelled = (data[\"Cancelled\"] > 0) | (data[\"Diverted\"] > 0)\n",
    "    completed_flights = data[~cancelled]\n",
    "\n",
    "    training_data = completed_flights[[\"DepDelay\", \"TaxiOut\", \"Distance\"]]\n",
    "    # Consider flights that arrive more than 15 min late as delayed\n",
    "    training_data[\"target\"] = completed_flights[\"ArrDelay\"] > 15\n",
    "\n",
    "    test_data = training_data[completed_flights[\"FlightDate\"] >= split_date]\n",
    "    training_data = training_data[completed_flights[\"FlightDate\"] < split_date]\n",
    "\n",
    "    training_data.to_csv(dataset_train.path, index=False)\n",
    "    test_data.to_csv(dataset_test.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9f077d25-f7de-4464-a647-a759aff1d772",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\",\n",
    ")\n",
    "def model_train(\n",
    "    dataset: Input[Dataset],\n",
    "    model: Output[Artifact],\n",
    "):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    data = pd.read_csv(dataset.path)\n",
    "    X = data.drop(columns=[\"target\"])\n",
    "    y = data[\"target\"]\n",
    "\n",
    "    model_pipeline = Pipeline(\n",
    "        [\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"clf\", LogisticRegression(random_state=42, tol=0.0001, max_iter=100)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model_pipeline.fit(X, y)\n",
    "\n",
    "    model.metadata[\"framework\"] = \"scikit-learn\"\n",
    "    model.metadata[\"containerSpec\"] = {\n",
    "        \"imageUri\": \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\"\n",
    "    }\n",
    "\n",
    "    file_name = model.path + \"/model.pkl\"\n",
    "    import pathlib\n",
    "\n",
    "    pathlib.Path(model.path).mkdir()\n",
    "    with open(file_name, \"wb\") as file:\n",
    "        pickle.dump(model_pipeline, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a82f9bd6-4cd8-43c7-bbf7-7758784a9c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\",\n",
    ")\n",
    "def model_evaluate(\n",
    "    test_set: Input[Dataset],\n",
    "    model: Input[Model],\n",
    "    metrics: Output[ClassificationMetrics],\n",
    "):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from sklearn.metrics import roc_curve, confusion_matrix, accuracy_score\n",
    "\n",
    "    data = pd.read_csv(test_set.path)[:1000]\n",
    "    file_name = model.path + \"/model.pkl\"\n",
    "    with open(file_name, \"rb\") as file:\n",
    "        model_pipeline = pickle.load(file)\n",
    "\n",
    "    X = data.drop(columns=[\"target\"])\n",
    "    y = data.target\n",
    "    y_pred = model_pipeline.predict(X)\n",
    "\n",
    "    y_scores = model_pipeline.predict_proba(X)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_true=y, y_score=y_scores, pos_label=True)\n",
    "    metrics.log_roc_curve(fpr.tolist(), tpr.tolist(), thresholds.tolist())\n",
    "\n",
    "    metrics.log_confusion_matrix(\n",
    "        [\"False\", \"True\"],\n",
    "        confusion_matrix(y, y_pred).tolist(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ec1d89d5-4eef-41b5-a222-87ce6e8b12de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the workflow of the pipeline.\n",
    "@pipeline(name=\"gcp-mlops-v0\", pipeline_root=pipeline_root_path)\n",
    "def pipeline(\n",
    "    training_data_url: str = f\"gs://{BUCKET}/data/processed/2020/2020-05.csv\",\n",
    "    test_split_date: str = \"2020-05-20\",\n",
    "):\n",
    "    data_op = data_download(\n",
    "        data_url=training_data_url,\n",
    "        split_date=test_split_date\n",
    "    )\n",
    "\n",
    "    from google_cloud_pipeline_components.experimental.custom_job.utils import (\n",
    "        create_custom_training_job_op_from_component,\n",
    "    )\n",
    "\n",
    "    custom_job_distributed_training_op = create_custom_training_job_op_from_component(\n",
    "        model_train, \n",
    "        replica_count=1, \n",
    "        machine_type = 'n1-standard-8'\n",
    "    )\n",
    "\n",
    "    model_train_op = custom_job_distributed_training_op(\n",
    "        dataset=data_op.outputs[\"dataset_train\"],\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "    )\n",
    "\n",
    "    model_evaluate_op = model_evaluate(\n",
    "        test_set=data_op.outputs[\"dataset_test\"],\n",
    "        model=model_train_op.outputs[\"model\"],\n",
    "    )\n",
    "\n",
    "    model_upload_op = ModelUploadOp(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        display_name=\"flight-delay-model\",\n",
    "        unmanaged_container_model=model_train_op.outputs[\"model\"],\n",
    "    ).after(model_evaluate_op)\n",
    "\n",
    "    endpoint_create_op = EndpointCreateOp(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        display_name=\"flight-delay-endpoint9\",\n",
    "    )\n",
    "\n",
    "    ModelDeployOp(\n",
    "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "        model=model_upload_op.outputs[\"model\"],\n",
    "        deployed_model_display_name=\"flight-delay-model\",\n",
    "        dedicated_resources_machine_type=\"n1-standard-8\",\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=3,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4f3496-8d3e-4525-834d-e9904375480f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/662390005506/locations/us-central1/pipelineJobs/gcp-mlops-v0-20230423175031\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/662390005506/locations/us-central1/pipelineJobs/gcp-mlops-v0-20230423175031')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/gcp-mlops-v0-20230423175031?project=662390005506\n",
      "PipelineJob projects/662390005506/locations/us-central1/pipelineJobs/gcp-mlops-v0-20230423175031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/662390005506/locations/us-central1/pipelineJobs/gcp-mlops-v0-20230423175031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/662390005506/locations/us-central1/pipelineJobs/gcp-mlops-v0-20230423175031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/662390005506/locations/us-central1/pipelineJobs/gcp-mlops-v0-20230423175031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/662390005506/locations/us-central1/pipelineJobs/gcp-mlops-v0-20230423175031 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"gcp-mlops-v0.json\")\n",
    "\n",
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET, location=REGION)\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"gcp-mlops-v0\",\n",
    "    template_path=\"gcp-mlops-v0.json\",\n",
    "    pipeline_root=pipeline_root_path,\n",
    ")\n",
    "\n",
    "job.run(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e3cdc3-0984-494d-9341-3fba164f5c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae2357c-93fe-4c52-8ada-548e53036982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fe7915ef-b490-4eee-bc94-4a46b9ae3a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is: Prediction(predictions=[False], deployed_model_id='2932338137650692096', model_version_id='1', model_resource_name='projects/662390005506/locations/us-central1/models/4506214266021347328', explanations=None)\n"
     ]
    }
   ],
   "source": [
    "# predictions from Python\n",
    "\n",
    "ENDPOINT_ID = '2557670754392997888'\n",
    "# get it from gcloud ai endpoints list. gcloud config set project polished-vault-379315. \n",
    "\n",
    "from google.cloud import aiplatform as aip\n",
    "\n",
    "aip.init(project=PROJECT_ID, location=REGION)\n",
    "endpoint = aip.Endpoint(ENDPOINT_ID)\n",
    "prediction = endpoint.predict(instances=[[-4.0, 16.0, 153.0]])\n",
    "print(f'Prediction is: {prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f181314-5165-408c-9e4f-0b04eec1bfaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3469231046.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/tmp/ipykernel_463/3469231046.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    nano INPUT.json\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# use the code below in Shell to test the endpoint.\n",
    "\n",
    "# gcloud auth application-default login\n",
    "nano INPUT.json\n",
    "\n",
    "{\n",
    "  \"instances\": [{1, 15, 400}]\n",
    "}\n",
    "\n",
    "ENDPOINT_ID=\"3891580669024796672\"\n",
    "PROJECT_ID=\"polished-vault-379315\"\n",
    "INPUT_DATA_FILE=\"INPUT.json\"\n",
    "\n",
    "curl \\\n",
    "-X POST \\\n",
    "-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-central1/endpoints/${ENDPOINT_ID}:predict \\\n",
    "-d \"@${INPUT_DATA_FILE}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bdd010-0b5b-4a30-8eab-971b34313769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86748e80-1049-4f84-936e-8406c382376f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be54b8c0-083b-4ec0-b8c8-9e2cc76ae54d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5411266c-7dc0-4551-a659-cba61498b273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65676f5a-2ed3-4b3b-9ff9-c45e1c26cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_url: str = f\"gs://{BUCKET}/data/processed/2020/2020-05.csv\"\n",
    "training_data_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aa88dc-d320-4438-bfc9-4120f19b65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(training_data_url, nrows=2000)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acccd9c3-afc1-411f-b2c6-b9f9b51ce7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data[[\"DepDelay\", \"TaxiOut\", \"Distance\"]]\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1981492-e3ee-4c13-b0d4-143abb0bc1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7946099-e44f-4894-8069-87bb57df2e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125c657d-93eb-48d3-a88d-25f4cbbf2592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bff380-93e5-49be-9511-57909ba162dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e077d24-d517-42fe-837c-139ef62a5dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url: str = f\"gs://{BUCKET}/data/2021/2021-12raw.csv\"\n",
    "data_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ff8ff1-e1f6-4c28-8f72-959c14da8674",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('gs://mpg3-temp-data/data/processed/2021/2021-12.csv', nrows=10000)\n",
    "df.head(1)\n",
    "# the file is the problem. if I download it directly via datapull, things are fine.\n",
    "# if I load file, preprocessed with R locally, I got this error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccfeb44-42aa-4fc7-86b0-165d520be310",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('gs://training-data-polished-vault-379315/data/2021/2021-12.csv', nrows=10)\n",
    "df = pd.read_csv('gs://training-data-polished-vault-379315/data/2021/2021-12.csv')\n",
    "df.head(2)\n",
    "# df = pd.read_csv('gs://mpg3-temp-data/data/2021/2021-12.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c06014e-aa93-492c-8d84-0049cd01c6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('gs://mpg3-temp-data/data/processed/2021/2021-12.csv', nrows=10000)\n",
    "df.head(1)\n",
    "# the file is the problem. if I download it directly via datapull, things are fine.\n",
    "# if I load file, preprocessed with R locally, I got this error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3012981c-29d3-4d02-9982-f926fedb6158",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m107",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m107"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
